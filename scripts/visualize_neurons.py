import argparse
from collections import Counter
import itertools
import json
import logging
import ntpath
import os
import random
import sys

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import torch
from torch import nn
import torch.nn.functional as F

sys.path.append(os.path.join(os.path.dirname(__file__), "../"))

logger = logging.getLogger(__name__)


def main():
    if args.target_index == "first":
        args.target_index = -(args.context_length - 1)
    elif args.target_index == "last":
        args.target_index = 0
    elif args.target_index == "middle":
        assert args.context_length % 2 == 0
        args.target_index = -(int(args.context_length / 2) - 1)
    else:
        args.target_index = -int(args.target_index)

    logger.info("Input Arguments:")
    print(json.dumps(vars(args), indent=4, sort_keys=True))

    # Set the random seed manually for reproducibility.
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        if not args.cuda:
            logger.warning("GPU available but not running with "
                           "CUDA (use --cuda to turn on.)")
        else:
            torch.cuda.manual_seed(args.seed)

    if args.mode == "uniform":
        # Make the uniform data.
        data = torch.LongTensor(args.context_length).random_(
            0, args.vocab_size)
    else:
        # Read the training data and index it.
        word2idx = {}
        train_data_list = []
        logger.info("Reading and indexing train data at {}".format(
            args.train_path))
        train_frequency_counter = Counter()
        with open(args.train_path) as train_file:
            for line in train_file:
                words = line.split() + ['<eos>']
                # Index words if they haven't been indexed
                for word in words:
                    train_frequency_counter[word] += 1
                    if word not in word2idx:
                        word2idx[word] = len(word2idx)
                # Add the indexed words to the train data
                train_data_list.extend([word2idx[word] for word in words])

        if args.mode == "adversarial":
            # Make the adversarial data.
            (most_common_word,
             highest_count) = train_frequency_counter.most_common(1)[0]
            logger.info(
                "Most common token in train set is {}, frequency {}".format(
                    most_common_word,
                    highest_count / sum(list(train_frequency_counter.values()))))

            num_least_common = 100
            least_common_words = [
                word2idx[word] for word, count in
                train_frequency_counter.most_common()[:-num_least_common - 1:-1]]
            np_data = np.random.choice(
                a=np.array(least_common_words),
                size=(args.context_length,),
                replace=True)
            data = torch.LongTensor(np_data)
        elif args.mode == "language":
            # Make the language data
            data = index_evaluation_data(args.test_path, word2idx, args.context_length,
                                         shuffle=False)
        else:
            # Make the ngram data
            data = index_evaluation_data(args.test_path, word2idx, args.context_length,
                                         shuffle_ngram=mode_to_ngram[args.mode])

    label = data[args.target_index - 1].item()
    # Check shapes
    assert len(data.size()) == 1
    assert data.size(0) == args.context_length

    logger.info("Loading saved model at {}".format(args.load_model))
    device = torch.device("cuda:0" if args.cuda else "cpu")
    with open(args.load_model, "rb") as model_file:
        model = torch.load(model_file, map_location=lambda storage, loc: storage)
    model = model.to(device)
    data = data.to(device)

    model_lstmcell = nn.LSTMCell(model.rnn.input_size, model.rnn.hidden_size)
    # Load weights into LSTMCell
    lstmcell_state_dict = {
        "weight_ih": model.rnn.state_dict()["weight_ih_l0"],
        "weight_hh": model.rnn.state_dict()["weight_hh_l0"],
        "bias_ih": model.rnn.state_dict()["bias_ih_l0"],
        "bias_hh": model.rnn.state_dict()["bias_hh_l0"]
    }
    model_lstmcell.load_state_dict(lstmcell_state_dict)
    decoder = model.decoder

    with torch.no_grad():
        # Embed the data. Shape: (1, context_length, embed_dim)
        embedded_data = model.embedding(data.unsqueeze(0))

        # Shape: (1, hidden_size)
        cell_states = []
        (hidden_state, cell_state) = init_hidden(model.rnn, embedded_data.size(0))
        for embedded_word in embedded_data.transpose(0, 1):
            # Embedded word shape: (batch_size (1), embedding_dim)
            hidden_state, cell_state = model_lstmcell(
                embedded_word, (hidden_state, cell_state))
            cell_states.append(cell_state[0].data)

        output_distribution = F.softmax(decoder(hidden_state[0]), dim=-1)
        logit, prediction = torch.max(output_distribution, 0)
        print("Model prediction: {}".format(prediction.item()))
        print("Model softmax weight assigned: {}".format(logit.item()))
        print("True label: {}".format(label))

    # Extract the activations of interest
    activations = []
    second_activations = []
    for cell_state in cell_states:
        activations.append(cell_state[args.neuron_index])
        if args.second_neuron_index:
            second_activations.append(cell_state[args.second_neuron_index])
    sns.set()
    fig, ax = plt.subplots()
    activations_line = ax.plot(range(len(activations)), activations, color="b",
                               label="Neuron {}".format(args.neuron_index),
                               linewidth=2.0)
    ax.set_ylabel("Neuron {} Activation Value".format(args.neuron_index))
    ax.set_xlabel("Timestep")
    if args.second_neuron_index:
        ax.tick_params("y", colors="b")
        second_ax = ax.twinx()
        second_line = second_ax.plot(
            range(len(activations)), second_activations, color="red",
            label="Neuron {}".format(args.second_neuron_index),
            linestyle="--", linewidth=2.0)
        second_ax.set_ylabel(
            "Neuron {} Activation Value".format(
                args.second_neuron_index),
            color="red")
        second_ax.tick_params("y", colors="red")
        lns = activations_line + second_line
        labs = [l.get_label() for l in lns]
        ax.legend(lns, labs, loc="upper right")
        second_ax.grid(None)
        ax.set_ylabel("Neuron {} Activation Value".format(args.neuron_index),
                      color="blue")
        ax.set_title("Cell State Activation of Neuron {} and {} for "
                     "each timestep \n "
                     "Context Length {}, timestep to predict: {}, "
                     "Evaluation data: {}".format(
                         args.neuron_index, args.second_neuron_index,
                         args.context_length,
                         args.context_length - 1 - abs(args.target_index),
                         args.mode))
    else:
        ax.set_title("Cell State Activation of Neuron {} for each timestep \n "
                     "Context Length {}, timestep to predict: {}, "
                     "Evaluation data: {}".format(
                         args.neuron_index, args.context_length,
                         args.context_length - 1 - abs(args.target_index),
                         args.mode))

    if args.second_neuron_index:
        save_path = "{}_neurons_{}_{}.png".format(path_leaf(args.load_model),
                                                  args.neuron_index,
                                                  args.second_neuron_index)
    else:
        save_path = "{}_neuron_{}.png".format(path_leaf(args.load_model),
                                              args.neuron_index)
    plt.savefig(save_path)


def path_leaf(path):
    head, tail = ntpath.split(path)
    return tail or ntpath.basename(head)


def init_hidden(rnn, batch_size):
    hidden_size = rnn.hidden_size
    weight = next(rnn.parameters()).data
    return (weight.new(batch_size, hidden_size).zero_(),
            weight.new(batch_size, hidden_size).zero_())


def index_evaluation_data(evaluation_path, word2idx, context_length,
                          shuffle_ngram=0):
    evaluation_data = []
    with open(evaluation_path) as evaluation_file:
        for line in evaluation_file:
            words = line.split() + ['<eos>']
            # Add the indexed words to the evaluation data
            evaluation_data.extend([word2idx[word] for word in words])
    if shuffle_ngram > 0:
        # Shuffle evaluation data
        logger.info("Shuffling evaluation data with ngram {}".format(
            shuffle_ngram))
        random.shuffle(evaluation_data)
        # Group words into tuples of size n.
        grouped_evaluation_data = [
            tuple(evaluation_data[i: i + mode_to_ngram[args.mode]]) for i in
            range(0, len(evaluation_data), mode_to_ngram[args.mode])]
        # Shuffle this list of tuples and then unpack to get
        # ngram train dataset.
        random.shuffle(grouped_evaluation_data)
        evaluation_data = list(itertools.chain.from_iterable(
            grouped_evaluation_data))

    # Turn evaluation data into a Tensor
    evaluation_data = torch.LongTensor(evaluation_data)[0: context_length]
    return evaluation_data


if __name__ == "__main__":
    logging.basicConfig(format="%(asctime)s - %(levelname)s "
                        "- %(name)s - %(message)s",
                        level=logging.INFO)
    # Path to directory that this file is in
    project_root = os.path.abspath(os.path.realpath(os.path.join(
        os.path.dirname(os.path.realpath(__file__)), os.pardir)))

    parser = argparse.ArgumentParser(
        description=("Train a RNN to predict the past, but with random "
                     "embeddings. Can optionally choose to freeze the "
                     "embeddings or output layer."),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--load-model", type=str, required=True,
                        help=("A model to load and analyze."))
    parser.add_argument("--train-path", type=str,
                        default=os.path.join(project_root, "data",
                                             "train.txt"),
                        help=("Create training data from this corpus. "
                              "If this is set,  --vocab-size will "
                              "be ignored"))
    parser.add_argument("--test-path", type=str,
                        default=os.path.join(project_root, "data",
                                             "valid.txt"),
                        help=("Create test data from this corpus. If this "
                              "is set, --train-path must be set as well."))
    parser.add_argument("--mode", type=str, required=True, default="adversarial",
                        choices=["uniform", "adversarial", "language", "unigram",
                                 "bigram", "trigram", "tetragram", "5gram",
                                 "10gram", "50gram"],
                        help=("How to generate the data fed into the RNN. "
                              "uniform indicates that the data is sampled "
                              "uniformly. language indicates that the "
                              "data is taken from the file at "
                              "--train-path. unigram indicates that the "
                              "data is sampled from the --train-path "
                              "unigram distribution, and bigram "
                              "/ trigram / tetragram accordingly follow."))
    parser.add_argument("--neuron-index", type=int, required=True,
                        help=("The index of the neuron to visualize."))
    parser.add_argument("--second-neuron-index", type=int,
                        help=("The index of another neuron to "
                              "optionally simultaneously visualize."))
    parser.add_argument("--target-index", type=str, default="middle",
                        help=("The index of the input history to predict. "
                              "0 is the last token in the sequence (most "
                              "recently seen token), -1 is the penultimate, "
                              "-2 is the 2nd to last, etc. If a positive "
                              "number is provided, then it is negated."
                              "If \"last\", we predict the last token. "
                              "If \"middle\", we  predict the middle token. "
                              "If \"first\", we predict the first token."))
    parser.add_argument("--context-length", type=int, required=True,
                        help=("Input sequence lengths."))
    parser.add_argument("--vocab-size", type=int, default=10000,
                        help=("The number of unique tokens in "
                              "the synthetic vocabulary."))
    parser.add_argument("--seed", type=int, default=0,
                        help="Random seed to use in training.")
    parser.add_argument("--cuda", action="store_true",
                        help="Use the GPU.")
    args = parser.parse_args()
    mode_to_ngram = {"unigram": 1, "bigram": 2, "trigram": 3,
                     "tetragram": 4, "5gram": 5, "10gram": 10,
                     "50gram": 50}
    main()
