import argparse
from collections import Counter
import json
import logging
import os
import operator
import random
import sys

import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
from sklearn.linear_model import LinearRegression

sys.path.append(os.path.join(os.path.dirname(__file__), "../"))

logger = logging.getLogger(__name__)


def main():
    if args.target_index == "first":
        args.target_index = -(args.context_length - 1)
    elif args.target_index == "last":
        args.target_index = 0
    elif args.target_index == "middle":
        assert args.context_length % 2 == 0
        args.target_index = -(int(args.context_length / 2) - 1)
    else:
        args.target_index = -int(args.target_index)

    logger.info("Input Arguments:")
    print(json.dumps(vars(args), indent=4, sort_keys=True))

    # Set the random seed manually for reproducibility.
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        if not args.cuda:
            logger.warning("GPU available but not running with "
                           "CUDA (use --cuda to turn on.)")
        else:
            torch.cuda.manual_seed(args.seed)

    word2idx = {}
    train_data_list = []
    logger.info("Reading and indexing train data at {}".format(
        args.train_path))
    train_frequency_counter = Counter()
    with open(args.train_path) as train_file:
        for line in train_file:
            words = line.split() + ['<eos>']
            for word in words:
                # Get frequencies for each of the words.
                train_frequency_counter[word] += 1
                # Assign each word an index.
                if word not in word2idx:
                    word2idx[word] = len(word2idx)
            # Add the indexed words to the train data.
            train_data_list.extend([word2idx[word] for word in words])

    # Create the uniform data
    if args.mode == "uniform":
        logger.info("Creating uniformly sampled data."
                    "{} context length, {} vocab size".format(
                        args.context_length, args.vocab_size))
        # Set Vocabulary size
        vocab_size = args.vocab_size

        # Generate the data indices
        data = torch.LongTensor(
            args.num_examples, args.context_length).random_(
                0, vocab_size)
        # Generate train labels by slicing.
        labels = data[:, args.target_index - 1]
        # Wrap the data and targets in TensorDataset.
        dataset = TensorDataset(data, labels)
    # Create the adversarial data.
    else:
        num_least_common = 100
        least_common_words = [
            word2idx[word] for word, count in
            train_frequency_counter.most_common()[:-num_least_common - 1:-1]]
        adversarial_np_data = np.random.choice(
            a=np.array(least_common_words),
            size=(args.num_examples, args.context_length),
            replace=True)
        # Turn test data into a LongTensor. shape: (num_examples, context_len).
        data = torch.LongTensor(adversarial_np_data)

    # Generate train labels by slicing.
    labels = data[:, args.target_index - 1]
    # Wrap the data and targets in TensorDataset.
    dataset = TensorDataset(data, labels)
    logger.info("Number of train examples: {}".format(len(dataset)))

    device = torch.device("cuda:0" if args.cuda else "cpu")
    # Load the best saved model.
    with open(args.load_model, "rb") as model_file:
        model = torch.load(model_file,
                           map_location=lambda storage, loc: storage)
    model = model.to(device)

    # Run the dataset through the RNN and collect
    # hidden states at each timestep.
    dataloader = DataLoader(dataset, batch_size=args.batch_size,
                            shuffle=False, num_workers=args.num_workers,
                            pin_memory=args.cuda)

    if type(model.rnn).__name__ != "LSTM":
        raise ValueError("This script is only compatible with LSTMs for now.")

    logger.info("Extracting cell states for test data.")
    model_lstmcell = nn.LSTMCell(model.rnn.input_size, model.rnn.hidden_size)
    # Load weights into LSTMCell
    lstmcell_state_dict = {
        "weight_ih": model.rnn.state_dict()["weight_ih_l0"],
        "weight_hh": model.rnn.state_dict()["weight_hh_l0"],
        "bias_ih": model.rnn.state_dict()["bias_ih_l0"],
        "bias_hh": model.rnn.state_dict()["bias_hh_l0"]
    }
    model_lstmcell.load_state_dict(lstmcell_state_dict)
    model_lstmcell = model_lstmcell.to(device)

    all_cell_states = []
    all_labels = []

    with torch.no_grad():
        for batch_idx, data_tuple in tqdm(enumerate(dataloader)):
            data, targets = data_tuple
            data = data.to(device)

            # Embed the data. Shape: (batch_size, context_length,
            #                         embedding_dim)
            embedded_data = model.embedding(data)

            # Shape: (num_layers, batch_size, hidden_size)
            (hidden_state, cell_state) = init_hidden(
                model.rnn, embedded_data.size(0))

            # Run the data through RNN.
            for idx, embedded_word in enumerate(embedded_data.transpose(0, 1)):
                # Embedded word shape: (batch_size, embedding_dim)
                hidden_state, cell_state = model_lstmcell(
                    embedded_word, (hidden_state, cell_state))
                all_cell_states.extend([x.detach().tolist() for
                                        x in cell_state])
                all_labels.extend([idx + 1 for x in cell_state])

    assert len(all_cell_states) == len(all_labels)
    # Filter all cell states and all labels to only include cell
    # states correspoding to labels that are less than or equal to
    # args.context_length - abs(args.target_index)
    regression_cell_states = []
    regression_labels = []
    for cell_state, label in zip(all_cell_states, all_labels):
        if label <= args.context_length - abs(args.target_index):
            regression_cell_states.append(cell_state)
            regression_labels.append(label)
    assert len(regression_cell_states) == len(regression_labels)

    # Try to fit this initial linear regression
    logger.info("Fitting linear regression from entire cell "
                "state vector to timestep information. "
                "{} examples in total".format(len(regression_cell_states)))
    np_regression_cell_states = np.array(regression_cell_states)
    np_regression_labels = np.array(regression_labels)
    all_neurons_linear_regression = LinearRegression()
    all_neurons_linear_regression.fit(np_regression_cell_states,
                                      np_regression_labels)
    all_neurons_r2 = all_neurons_linear_regression.score(
        np_regression_cell_states, np_regression_labels)
    print("R^2 of model when using entire cell state vector "
          "to predict timestep information: {}".format(all_neurons_r2))

    logger.info("Fitting linear regression from each neuron to "
                "timestep information")
    # List of lists. Length of outer list: number of neurons.
    # Length of inner list: # of examples
    num_hidden_neurons = len(regression_cell_states[0])
    regression_neuron_activations = []
    for i in range(num_hidden_neurons):
        regression_neuron_activations.append(
            [cell_state[i] for cell_state in regression_cell_states])
    assert len(regression_neuron_activations) == num_hidden_neurons
    assert len(regression_neuron_activations[0]) == len(regression_labels)

    # For each neuron, fit a linear regression
    neuron_r2s = {}
    for idx, neuron_activations in enumerate(regression_neuron_activations):
        np_neuron_activations = np.array(neuron_activations).reshape(-1, 1)
        neuron_r2 = LinearRegression().fit(
            np_neuron_activations, np_regression_labels).score(
                np_neuron_activations, np_regression_labels)
        neuron_r2s[idx] = neuron_r2

    logger.info("Top 10 neurons indices and their R^2 to timestep information")
    for idx, r2 in sorted(neuron_r2s.items(), key=operator.itemgetter(1),
                          reverse=True)[:10]:
        print("Index: {}, R2: {}".format(idx, r2))


def init_hidden(rnn, batch_size):
    hidden_size = rnn.hidden_size
    weight = next(rnn.parameters()).data
    return (weight.new(batch_size, hidden_size).zero_(),
            weight.new(batch_size, hidden_size).zero_())


def index_evaluation_data(evaluation_path, word2idx, context_length,
                          target_index, shuffle):
    evaluation_data = []
    with open(evaluation_path) as evaluation_file:
        for line in evaluation_file:
            words = line.split() + ['<eos>']
            # Add the indexed words to the train data
            evaluation_data.extend([word2idx[word] for word in words])
    if shuffle:
        # Shuffle evaluation data
        random.shuffle(evaluation_data)
    # Turn evaluation data into a Tensor
    evaluation_data = torch.LongTensor(evaluation_data)
    # Turn evaluation data into examples. shape: (num_examples, context_len)
    evaluation_data = torch.stack(
        [evaluation_data[i: i + context_length] for i in
         range(len(evaluation_data) - context_length + 1)])
    return evaluation_data


if __name__ == "__main__":
    logging.basicConfig(format="%(asctime)s - %(levelname)s "
                        "- %(name)s - %(message)s",
                        level=logging.INFO)

    # Path to project root directory
    project_root = os.path.abspath(os.path.realpath(os.path.join(
        os.path.dirname(os.path.realpath(__file__)), os.pardir)))

    parser = argparse.ArgumentParser(
        description=("Train a RNN to predict the past, but with random "
                     "embeddings. Can optionally choose to freeze the "
                     "embeddings or output layer."),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--load-model", type=str,
                        help=("A model to load and evaluate on test data."))
    parser.add_argument("--train-path", type=str,
                        default=os.path.join(project_root, "data",
                                             "valid.txt"),
                        help=("Path to the data to use to find "
                              "counting neurons."))
    parser.add_argument("--mode", type=str,
                        choices=["uniform", "adversarial"], default="adversarial",
                        help=("The dataset to train the RNN on. uniform "
                              "indicates that the data is sampled uniformly. "
                              "Adversarial indicates that the data is sampled "
                              "uniformly from the 100 rarest tokens in the "
                              "train-path."))
    parser.add_argument("--num-examples", type=int, default=10000,
                        help=("The number of examples to use."))
    parser.add_argument("--target-index", type=str, default="middle",
                        help=("The index of the input history to predict. "
                              "0 is the last token in the sequence (most "
                              "recently seen token), -1 is the penultimate, "
                              "-2 is the 2nd to last, etc. If a positive "
                              "number is provided, then it is negated."
                              "If \"last\", we predict the last token. "
                              "If \"middle\", we  predict the middle token. "
                              "If \"first\", we predict the first token."))
    parser.add_argument("--context-length", type=int, required=True,
                        help=("The sequence length of the inputs"))
    parser.add_argument("--vocab-size", type=int, default=10000,
                        help=("The number of unique tokens in "
                              "the synthetic vocabulary."))
    parser.add_argument("--batch-size", type=int, default=128,
                        help="Batch size to use in the model.")
    parser.add_argument("--seed", type=int, default=0,
                        help="Random seed to use.")
    parser.add_argument("--num-workers", type=int, default=4,
                        help="The number of processes the use the load data.")
    parser.add_argument("--cuda", action="store_true",
                        help="Use the GPU.")

    args = parser.parse_args()
    main()
