import argparse
from collections import Counter
import itertools
import json
import logging
import math
import os
import random
import sys
import time

import numpy as np
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, TensorDataset

sys.path.append(os.path.join(os.path.dirname(__file__), "../"))
from lstms_exploit_linguistic_attributes.embedding_rnn import EmbeddingRNN

logger = logging.getLogger(__name__)


def main():
    if args.run_id is None and args.load_model is None:
        raise ValueError("run_id is required when training a model.")

    if args.target_index == "first":
        args.target_index = -(args.context_length - 1)
    elif args.target_index == "last":
        args.target_index = 0
    elif args.target_index == "middle":
        assert args.context_length % 2 == 0
        args.target_index = -(int(args.context_length / 2) - 1)
    else:
        args.target_index = -int(args.target_index)

    logger.info("Input Arguments:")
    print(json.dumps(vars(args), indent=4, sort_keys=True))

    # Set the random seed manually for reproducibility.
    random.seed(args.seed)
    torch.manual_seed(args.seed)

    device = torch.device("cuda:0" if args.cuda else "cpu")
    if torch.cuda.is_available():
        if not args.cuda:
            logger.warning("GPU available but not running with "
                           "CUDA (use --cuda to turn on.)")
        else:
            torch.cuda.manual_seed(args.seed)

    ################################
    # Create adversarial test set. #
    ################################
    # Read the training data and index it.
    word2idx = {}
    train_data_list = []
    logger.info("Reading and indexing train data at {}".format(
        args.train_path))
    train_frequency_counter = Counter()
    with open(args.train_path) as train_file:
        for line in train_file:
            words = line.split() + ['<eos>']
            for word in words:
                # Get frequencies for each of the words.
                train_frequency_counter[word] += 1
                # Assign each word an index.
                if word not in word2idx:
                    word2idx[word] = len(word2idx)
            # Add the indexed words to the train data.
            train_data_list.extend([word2idx[word] for word in words])

    (most_common_word,
     highest_count) = train_frequency_counter.most_common(1)[0]
    logger.info(
        "Most common token in train set is {}, frequency {}".format(
            most_common_word,
            highest_count / sum(list(train_frequency_counter.values()))))

    # Make adversarial test set out of the num_least_common tokens.
    num_least_common = 100
    num_adversarial_examples = 100000
    least_common_words = [
        word2idx[word] for word, count in
        train_frequency_counter.most_common()[:-num_least_common - 1:-1]]
    adversarial_np_test_data = np.random.choice(
        a=np.array(least_common_words),
        size=(num_adversarial_examples, args.context_length),
        replace=True)
    # Turn test data into a LongTensor. shape: (num_examples, context_len).
    adversarial_test_data = torch.LongTensor(adversarial_np_test_data)
    # Generate the labels by taking a slice of the data.
    adversarial_test_labels = adversarial_test_data[
        :, args.target_index - 1]
    # Wrap the data and targets in TensorDataset.
    adversarial_test_dataset = TensorDataset(adversarial_test_data,
                                             adversarial_test_labels)

    # Create the language data
    if args.mode == "language":
        # Set Vocabulary size
        vocab_size = len(word2idx)
        logger.info("Train dataset vocabulary size: {}".format(vocab_size))
        # Turn train data into a Tensor
        train_data = torch.LongTensor(train_data_list)
        # Turn train data into examples. Shape: (num_examples, context_len)
        train_data = torch.stack([
            train_data[i: i + args.context_length] for i in
            range(len(train_data) - args.context_length + 1)])
        # Generate the labels by taking a slice of the data.
        train_labels = train_data[:, args.target_index - 1]
        # Wrap the data and targets in TensorDataset.
        train_dataset = TensorDataset(train_data, train_labels)

        # Read the validation data and numericalize it.
        logger.info("Reading and numericalizing "
                    "validation data at {}".format(args.validation_path))
        validation_dataset = index_evaluation_data(
            args.validation_path, word2idx,
            args.context_length, args.target_index)

        # Read the test data and numericalize it.
        logger.info("Reading and numericalizing test "
                    "data at {}".format(args.test_path))
        test_dataset = index_evaluation_data(
            args.test_path, word2idx,
            args.context_length, args.target_index)

        logger.info("Number of train examples: {}".format(len(train_dataset)))
        logger.info("Number of validation examples: {}".format(
            len(validation_dataset)))
        logger.info("Number of test examples: {}".format(len(test_dataset)))
    elif args.mode == "uniform":
        logger.info("Creating uniformly sampled data."
                    "{} context length, {} vocab size".format(
                        args.context_length, args.vocab_size))
        # Set Vocabulary size
        vocab_size = args.vocab_size

        # Generate the data indices
        train_data = torch.LongTensor(
            args.num_train_examples, args.context_length).random_(
                0, vocab_size)
        # Generate train labels by slicing.
        train_labels = train_data[:, args.target_index - 1]
        # Wrap the data and targets in TensorDataset.
        train_dataset = TensorDataset(train_data, train_labels)

        # Generate the data indices
        validation_data = torch.LongTensor(
            args.num_validation_examples, args.context_length).random_(
                0, vocab_size)
        # Generate validation labels by slicing.
        validation_labels = validation_data[:, args.target_index - 1]
        # Wrap the data and targets in TensorDataset.
        validation_dataset = TensorDataset(validation_data, validation_labels)

        # Generate the data indices
        test_data = torch.LongTensor(
            args.num_test_examples, args.context_length).random_(
                0, vocab_size)
        # Generate test labels by slicing.
        test_labels = test_data[:, args.target_index - 1]
        # Wrap the data and targets in TensorDataset.
        test_dataset = TensorDataset(test_data, test_labels)

        logger.info("Number of train examples: {}".format(len(train_dataset)))
        logger.info("Number of validation examples: {}".format(
            len(validation_dataset)))
        logger.info("Number of test examples: {}".format(len(test_dataset)))
    else:
        # Set Vocabulary size
        vocab_size = len(word2idx)
        logger.info("Train dataset vocabulary size: {}".format(vocab_size))

        # Group words into tuples of size n.
        grouped_train_data_list = [
            tuple(train_data_list[i: i + mode_to_ngram[args.mode]]) for i in
            range(0, len(train_data_list), mode_to_ngram[args.mode])]
        # Shuffle this list of tuples and then unpack to get
        # ngram train dataset.
        random.shuffle(grouped_train_data_list)
        train_data_list = list(itertools.chain.from_iterable(
            grouped_train_data_list))
        train_data = torch.LongTensor(train_data_list)

        # Turn train data into examples. Shape: (num_examples, context_len)
        train_data = torch.stack([
            train_data[i: i + args.context_length] for i in
            range(len(train_data) - args.context_length + 1)])
        # Generate the labels by taking a slice of the data.
        train_labels = train_data[:, args.target_index - 1]
        # Wrap the data and targets in TensorDataset.
        train_dataset = TensorDataset(train_data, train_labels)

        # Read the validation data and numericalize it.
        logger.info("Reading and numericalizing "
                    "validation data at {}".format(args.validation_path))
        validation_dataset = index_evaluation_data(
            args.validation_path, word2idx,
            args.context_length, args.target_index,
            ngram=mode_to_ngram[args.mode])

        # Read the test data and numericalize it.
        logger.info("Reading and numericalizing test "
                    "data at {}".format(args.test_path))
        test_dataset = index_evaluation_data(
            args.test_path, word2idx,
            args.context_length, args.target_index,
            ngram=mode_to_ngram[args.mode])
    if args.load_model:
        # Load the best saved model.
        with open(args.load_model, "rb") as model_file:
            model = torch.load(model_file,
                               map_location=lambda storage, loc: storage)

        model.to(device)

        # Run on test data.
        test_loss, test_accuracy = evaluate(
            model, test_dataset, args.batch_size, args.cuda, args.num_workers)
        print('=' * 79)
        print("| End of training | test loss {:5.2f} | "
              "test ppl {:8.2f}".format(
                  test_loss, math.exp(test_loss)))
        print("test accuracy: {:4.3f}".format(test_accuracy))
        print('=' * 79)
        sys.exit()

    # If save_dir does not exist, create it
    save_dir = os.path.join(args.save_dir, args.run_id)
    if not os.path.exists(os.path.join(args.save_dir, args.run_id)):
        logger.info("Directory for serialization {} does not exist, "
                    "creating it".format(save_dir))
        os.makedirs(save_dir)

    # Save the input configuration
    config_path = os.path.join(save_dir, args.run_id + "_config.json")
    logger.info("Writing experiment configuration to {}".format(config_path))
    with open(config_path, "w") as config_file:
        json.dump(vars(args), config_file)
    # Create the model save path
    save_path = os.path.join(save_dir, args.run_id + ".th")
    logger.info("Will save best model checkpoint to {}".format(save_path))

    # Build the model
    logger.info("Building model")
    model = EmbeddingRNN(rnn_type=args.rnn_type,
                         vocab_size=vocab_size,
                         embedding_size=args.embedding_hidden_size,
                         hidden_size=args.embedding_hidden_size,
                         num_layers=args.num_layers,
                         batch_first=True,
                         dropout=args.dropout,
                         tied=True)
    model.to(device)

    # Create optimizer
    optimizers = {"adam": torch.optim.Adam,
                  "adagrad": torch.optim.Adagrad,
                  "sgd": torch.optim.SGD}

    model.embedding.weight.requires_grad = False
    for param in model.decoder.parameters():
        param.requires_grad = False

    optimizer = optimizers[args.optimizer](
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.lr)
    scheduler = ReduceLROnPlateau(
        optimizer, "min", factor=args.decay, verbose=True,
        patience=args.patience)

    total_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else
                       x.size()[0] for x in model.parameters())
    trainable_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else
                           x.size()[0] for x in model.parameters() if
                           x.requires_grad)
    print("Model total parameters:", total_params)
    print("Model trainable parameters:", trainable_params)

    # Loop over epochs.
    logger.info("Training model")
    best_val_loss = None
    print("-" * 79)
    print("| Starting epoch 1")
    print('-' * 79)
    for epoch in range(1, args.num_epochs + 1):
        if not args.fixed_dataset:
            # Regenerate the data, depending on what mode we're in.
            if args.mode == "uniform":
                logger.info("uniform mode: regenerating train data "
                            "by shuffling.")
                # Regenerate the train data and train labels.
                train_data = torch.LongTensor(
                    args.num_train_examples, args.context_length).random_(
                        0, vocab_size)
                train_labels = train_data[:, args.target_index - 1]
                train_dataset = TensorDataset(train_data, train_labels)
            elif args.mode == "language":
                logger.info("language mode: regenerating train data "
                            "by perturbing.")
                # Turn train data into a Tensor
                train_data = torch.LongTensor(train_data_list)
                # Pick the number of words to randomly remove from the start
                num_to_remove = random.randint(1, args.context_length)
                train_data = train_data[num_to_remove:]
                # Turn train data into examples.
                # shape: (num_examples, context_len)
                train_data = torch.stack([
                    train_data[i: i + args.context_length] for i in
                    range(len(train_data) - args.context_length + 1)])
                train_labels = train_data[:, args.target_index - 1]
                train_dataset = TensorDataset(train_data, train_labels)
            else:
                logger.info("{} mode: regenerating train data "
                            "by reshuffling.".format(args.mode))
                # Group words into tuples of size n.
                grouped_train_data_list = [
                    tuple(train_data_list[
                        i: i + mode_to_ngram[args.mode]]) for i in
                    range(0, len(train_data_list), mode_to_ngram[args.mode])]
                # Shuffle this list of tuples and then unpack to get
                # ngram train dataset.
                random.shuffle(grouped_train_data_list)
                train_data_list = list(itertools.chain.from_iterable(
                    grouped_train_data_list))
                train_data = torch.LongTensor(train_data_list)

                # Turn train data into examples.
                # Shape: (num_examples, context_len)
                train_data = torch.stack([
                    train_data[i: i + args.context_length] for i in
                    range(len(train_data) - args.context_length + 1)])
                # Generate the labels by taking a slice of the data.
                train_labels = train_data[:, args.target_index - 1]
                # Wrap the data and targets in TensorDataset.
                train_dataset = TensorDataset(train_data, train_labels)

        epoch_start_time = time.time()
        _train_one_epoch(
            model=model,
            train_dataset=train_dataset,
            batch_size=args.batch_size,
            clip_norm=args.clip_norm,
            log_interval=args.log_interval,
            optimizer=optimizer,
            cuda=args.cuda,
            num_workers=args.num_workers)
        val_loss, val_accuracy = evaluate(
            model, validation_dataset, args.batch_size, args.cuda,
            args.num_workers)
        print('-' * 79)
        print('| Finished epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
              'valid ppl {:8.2f}'.format(
                  epoch, (time.time() - epoch_start_time),
                  val_loss, math.exp(val_loss)))
        print("validation accuracy: {:4.3f}".format(val_accuracy))

        # Do adversarial evaluation
        adv_test_loss, adv_test_accuracy = evaluate(
            model, adversarial_test_dataset, args.batch_size,
            args.cuda, args.num_workers)
        print("| adversarial loss {:5.2f} | "
              "adversarial ppl {:8.2f}".format(
                  adv_test_loss, math.exp(adv_test_loss)))
        print("adversarial test accuracy: {:4.3f}".format(
            adv_test_accuracy))
        print('-' * 79)

        # Save the model if the validation loss is the best we've seen so far.
        if not best_val_loss or best_val_loss - val_loss > 0.0001:
            with open(save_path, 'wb') as f:
                torch.save(model, f)
            best_val_loss = val_loss

        # Update the learning rate
        scheduler.step(val_loss)
        if optimizer.param_groups[0]["lr"] < 0.00001:
            logger.info("Learning rate of {} is too small. "
                        "Stopping early!".format(
                            optimizer.param_groups[0]["lr"]))
            break

        if val_accuracy == 100 and val_loss < 0.001:
            logger.info("Val accuracy is 100 and "
                        "val_loss is {}, stopping".format(val_loss))
            break

    # Load the best saved model.
    with open(save_path, "rb") as model_file:
        model = torch.load(model_file,
                           map_location=lambda storage, loc: storage)
    model.to(device)
    # Run on test data.
    test_loss, test_accuracy = evaluate(
        model, test_dataset, args.batch_size, args.cuda, args.num_workers)
    print('=' * 79)
    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(
        test_loss, math.exp(test_loss)))
    print("test accuracy: {:4.3f}".format(test_accuracy))

    # Do adversarial evaluation
    adv_test_loss, adv_test_accuracy = evaluate(
        model, adversarial_test_dataset, args.batch_size,
        args.cuda, args.num_workers)
    print("| adversarial loss {:5.2f} | "
          "adversarial ppl {:8.2f}".format(
              adv_test_loss, math.exp(adv_test_loss)))
    print("adversarial test accuracy: {:4.3f}".format(
        adv_test_accuracy))
    print('=' * 79)


def _train_one_epoch(model, train_dataset, batch_size, clip_norm,
                     log_interval, optimizer, cuda, num_workers):
    """
    Train the RNN for one epoch.
    """
    device = torch.device("cuda:0" if cuda else "cpu")
    model.train()
    # Create dataloader
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,
                                  shuffle=True, num_workers=num_workers,
                                  pin_memory=cuda)

    criterion = nn.CrossEntropyLoss()
    log_interval_loss = 0
    num_examples = 0
    start_time = time.time()
    for batch_idx, data_tuple in enumerate(train_dataloader):
        data, targets = data_tuple
        data = data.to(device)
        targets = targets.to(device)

        optimizer.zero_grad()
        output_distribution = model(data)

        loss = criterion(output_distribution, targets)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)
        optimizer.step()

        # Multiply mean loss by batch size to get total sum loss.
        log_interval_loss += (loss.detach().item() * data.size(0))
        num_examples += len(targets)

        if batch_idx % log_interval == 0 and batch_idx > 0:
            cur_loss = log_interval_loss / num_examples
            elapsed = time.time() - start_time
            print('| {:5d}/{:5d} batches | lr {:02.4f} | '
                  'ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(
                      batch_idx, len(train_dataloader),
                      optimizer.param_groups[0]['lr'],
                      elapsed * 1000 / log_interval, cur_loss,
                      math.exp(cur_loss)))
            log_interval_loss = 0
            num_examples = 0
            start_time = time.time()


def evaluate(model, eval_dataset, batch_size, cuda, num_workers):
    """
    """
    device = torch.device("cuda:0" if cuda else "cpu")
    # Turn on evaluation mode which disables dropout.
    model.eval()

    criterion = nn.CrossEntropyLoss()

    total_loss = 0
    total_num_correct = 0
    num_batches = 0
    num_examples = 0

    # Create dataloader
    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size,
                                 shuffle=False, num_workers=num_workers,
                                 pin_memory=cuda)

    with torch.no_grad():
        for data_tuple in eval_dataloader:
            # data is (batch, context_length)
            data, targets = data_tuple
            data = data.to(device)
            targets = targets.to(device)

            output_distribution = model(data)
            _, predicted = torch.max(output_distribution, 1)
            total_num_correct += (predicted == targets).sum().item()
            total_loss += (criterion(output_distribution,
                                     targets).detach().item() *
                           data.size(0))
            num_batches += 1
            num_examples += len(targets)

    return (total_loss / num_examples,
            (total_num_correct / num_examples) * 100)


def index_evaluation_data(evaluation_path, word2idx, context_length,
                          target_index, ngram=None):
    evaluation_data = []
    with open(evaluation_path) as evaluation_file:
        for line in evaluation_file:
            words = line.split() + ['<eos>']
            # Add the indexed words to the evaluation data
            evaluation_data.extend([word2idx[word] for word in words])
    if ngram is not None:
        # Group evaluation data words into tuples of size ngram
        grouped_evaluation_data_list = [
            tuple(evaluation_data[i: i + ngram]) for i in
            range(0, len(evaluation_data), ngram)]
        # Shuffle this list of tuples and then unpack to
        # get ngram evaluation dataset.
        random.shuffle(grouped_evaluation_data_list)
        evaluation_data = list(itertools.chain.from_iterable(
            grouped_evaluation_data_list))

    # Turn evaluation data into a Tensor
    evaluation_data = torch.LongTensor(evaluation_data)
    # Turn evaluation data into examples. shape: (num_examples, context_len)
    evaluation_data = torch.stack([
        evaluation_data[i: i + context_length] for i in
        range(len(evaluation_data) - context_length + 1)])
    evaluation_labels = evaluation_data[:, target_index - 1]
    return TensorDataset(evaluation_data, evaluation_labels)


if __name__ == "__main__":
    logging.basicConfig(format="%(asctime)s - %(levelname)s "
                        "- %(name)s - %(message)s",
                        level=logging.INFO)
    # Path to project root.
    project_root = os.path.abspath(os.path.realpath(os.path.join(
        os.path.dirname(os.path.realpath(__file__)), os.pardir)))

    parser = argparse.ArgumentParser(
        description=("Train a RNN to predict a particular index "
                     "in its input history. The embeddings and "
                     "output are tied."),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--load-model", type=str,
                        help=("A model to load and evaluate on test data."))
    parser.add_argument("--run-id", type=str,
                        help=("A unique name for this run of the model. "
                              "This is used to name the saved model and "
                              "config file."))
    parser.add_argument("--train-path", type=str,
                        default=os.path.join(project_root, "data",
                                             "train.txt"),
                        help=("Train data."))
    parser.add_argument("--validation-path", type=str,
                        default=os.path.join(project_root, "data",
                                             "valid.txt"),
                        help=("Validation data."))
    parser.add_argument("--test-path", type=str,
                        default=os.path.join(project_root, "data", "test.txt"),
                        help=("Test data."))
    parser.add_argument("--mode", type=str,
                        choices=["uniform", "language", "unigram", "bigram",
                                 "trigram", "tetragram", "5gram", "10gram",
                                 "50gram"],
                        help=("The dataset to train the RNN on. uniform "
                              "indicates that the data is sampled uniformly. "
                              "language indicates that the data is taken from "
                              "the file at --train-path. unigram "
                              "indicates that the data is sampled from the "
                              "--train-path unigram distribution, and bigram "
                              "/ trigram / tetragram accordingly follow."))
    parser.add_argument("--num-train-examples", type=int, default=1000000,
                        help=("The number of train examples to create, "
                              "if mode is uniform of ngram."))
    parser.add_argument("--num-validation-examples", type=int, default=100000,
                        help=("The number of validation examples to create, "
                              "if mode is uniform of ngram."))
    parser.add_argument("--num-test-examples", type=int, default=100000,
                        help=("The number of test examples to create, "
                              "if mode is uniform of ngram."))
    parser.add_argument("--target-index", type=str, default="middle",
                        help=("The index of the input history to predict. "
                              "0 is the last token in the sequence (most "
                              "recently seen token), -1 is the penultimate, "
                              "-2 is the 2nd to last, etc. If a positive "
                              "number is provided, then it is negated."
                              "If \"last\", we predict the last token. "
                              "If \"middle\", we  predict the middle token. "
                              "If \"first\", we predict the first token."))
    parser.add_argument("--context-length", type=int, required=True,
                        help=("The sequence length of the inputs"))
    parser.add_argument("--vocab-size", type=int, default=10000,
                        help=("The number of unique tokens in "
                              "the synthetic vocabulary."))
    parser.add_argument("--rnn-type", type=str, default="LSTM",
                        choices=["LSTM", "GRU", "RNN_TANH", "RNN_RELU"],
                        help=("type of recurrent net (RNN_TANH, RNN_RELU, "
                              "LSTM, GRU)"))
    parser.add_argument("--embedding-hidden-size", type=int, required=True,
                        help=("Size of the word embeddings and RNN "
                              "hidden states."))
    parser.add_argument("--num-layers", type=int, default=1,
                        help="Number of layers to use in the RNN.")
    parser.add_argument("--optimizer", type=str, default="adam",
                        choices=["sgd", "adam", "adagrad"],
                        help="The optimizer to use.")
    parser.add_argument("--lr", type=float, default=0.001,
                        help="initial learning rate")
    parser.add_argument("--decay", type=float, default=0.5,
                        help=("Whenever validation accuracy does not "
                              "increase, set the learning rate to "
                              "decay * current learning rate."))
    parser.add_argument("--patience", type=int, default=3,
                        help=("Number of epochs of no improvement "
                              "after which decay will be applied."))
    parser.add_argument("--clip-norm", type=float, default=5.0,
                        help="Threshold for gradient clipping.")
    parser.add_argument("--num-epochs", type=int, default=240,
                        help="The number of epochs to train for.")
    parser.add_argument("--batch-size", type=int, default=128,
                        help="Batch size to use in the model.")
    parser.add_argument("--dropout", type=float, default=0.0,
                        help="Dropout to use in the model.")
    parser.add_argument("--seed", type=int, default=0,
                        help="Random seed to use in training.")
    parser.add_argument("--num-workers", type=int, default=4,
                        help="The number of processes the use the load data.")
    parser.add_argument("--cuda", action="store_true",
                        help="Use the GPU.")
    parser.add_argument("--log-interval", type=int, default=200,
                        help="How often to log progress.")
    parser.add_argument("--fixed-dataset", action="store_true",
                        help=("Do not regenerate data at the "
                              "start of each epoch."))
    parser.add_argument("--save-dir", type=str,
                        default=os.path.join(project_root, "models"),
                        help=("Path to directory to save the model and "
                              "config. Directory will be created if it "
                              "does not exist."))

    args = parser.parse_args()
    mode_to_ngram = {"unigram": 1, "bigram": 2, "trigram": 3,
                     "tetragram": 4, "5gram": 5, "10gram": 10,
                     "50gram": 50}
    main()
