import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence as pack
from torch.nn.utils.rnn import pad_packed_sequence as pad


class EmbeddingRNN(nn.Module):
    """
    This Module is a simple wrapper around a RNN that
    accepts an embedding matrix and variable-length input
    with token indices.
    """
    def __init__(self, rnn_type, vocab_size, embedding_size, hidden_size,
                 num_layers, batch_first, dropout, bidirectional=False,
                 tied=False):
        """
        Parameters
        ----------
        rnn_type: str
             The type of RNN to use. Available options are "LSTM" for a LSTM,
             "GRU" for a GRU, "RNN_TANH" for a Elman RNN with tanh activation,
             and "RNN_RELU" for a Elman RNN with ReLU activation.

        vocab_size: int
            The number of types in the vocabulary.

        embedding_size: int
            The dimension of the embeddings.

        hidden_size: int
            The dimension of the hidden state vector.

        num_layers: int
            The number of layers to use in the RNN.

        batch_first: boolean
            If True, then the input and output tensors are
            provided as (batch, seq, feature).

        dropout: float
            The dropout proportion applied to RNN layers, the embedding,
            and the RNN output.

        bidirectional: boolean, optional (default=False)
            Whether to use a bidirectional RNN. Only valid if the class
            is a FixedContextRNNLM

        tied: boolean, optional (default=False)
            Whether or not to tie the embedding and the output embedding
            weights.
        """
        super(EmbeddingRNN, self).__init__()
        self.rnn_type = rnn_type
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.batch_first = batch_first
        self.dropout = nn.Dropout(dropout)
        self.bidirectional = bidirectional

        self.vocab_size = vocab_size
        self.embedding_size = embedding_size
        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)

        self.bidirectional = bidirectional

        if rnn_type in ["LSTM", "GRU"]:
            # Create a LSTM or GRU
            self.rnn = getattr(nn, rnn_type)(
                input_size=self.embedding_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=self.batch_first,
                dropout=dropout,
                bidirectional=self.bidirectional)
        elif rnn_type in ["RNN_TANH", "RNN_RELU"]:
            # Create an Elman RNN with the specified activation function.
            nonlinearity = {"RNN_TANH": "tanh", "RNN_RELU": "relu"}[rnn_type]
            self.rnn = nn.RNN(
                input_size=self.embedding_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                nonlinearity=nonlinearity,
                batch_first=self.batch_first,
                dropout=dropout,
                bidirectional=self.bidirectional)
        else:
            raise ValueError("An invalid rnn_type {} was specified. "
                             "Options are \"LSTM\", \"GRU\", \"RNN_TANH\", "
                             "or \"RNN_RELU\".".format(rnn_type))

        if self.bidirectional:
            self.rnn_output_size = self.hidden_size * 2
        else:
            self.rnn_output_size = self.hidden_size
        self.decoder = nn.Linear(self.rnn_output_size, self.vocab_size)

        self.tied = tied
        if self.tied:
            if self.hidden_size != self.embedding_size:
                raise ValueError("When using the tied flag, hidden "
                                 "size must equal embedding size.")
            self.decoder.weight = self.embedding.weight

    def forward(self, inputs, lengths=None):
        """
        Parameters
        ----------
        inputs: LongTensor
            The input data. Shape is (seq_len, batch_size) if
            batch_first=False, or (batch_size, seq_len) if
            batch_first=True.

        lengths: LongTensor or List[int], optional
            List of integers with the sequence length for each
            element in the batch.

        Returns
        -------
        output_distribution: FloatTensor
            FloatTensor of shape (batch_size, vocab_size),
            which is a distribution over the vocabulary for each batch.

        """
        # Shape: (batch_size, seq_len, embedding_size) if batch_first=True
        # Shape: (batch_size, seq_len, embedding_size) if batch_first=False
        embedded_seq = self.embedding(inputs)
        embedded_seq = self.dropout(embedded_seq)

        if lengths is not None:
            # Pack the sequence
            embedded_seq = pack(embedded_seq, lengths,
                                batch_first=self.batch_first)

        # encoded_seq shape if batch_first=True: (batch_size, seq_len,
        #                                         hidden_size)
        # encoded_seq shape if batch_first=False: (seq_len, batch_size,
        #                                          hidden_size)
        self.rnn.flatten_parameters()
        encoded_seq, _ = self.rnn(embedded_seq)

        if lengths is not None:
            # Pad the packed sequence
            encoded_seq, _ = pad(encoded_seq,
                                 batch_first=self.batch_first)

        # Apply dropout.
        encoded_seq = self.dropout(encoded_seq)

        # Get the output after encoding the entire sequence
        if lengths is not None:
            # Shape: (batch_size, hidden_size)
            idx = (torch.tensor(
                lengths, dtype=torch.long,
                device=encoded_seq.device) - 1).view(-1, 1).expand(
                len(lengths), encoded_seq.size(2))
            time_dimension = 1 if self.batch_first else 0
            idx = idx.unsqueeze(time_dimension)
            last_output = encoded_seq.gather(
                time_dimension, idx).squeeze(time_dimension)
        else:
            last_output = encoded_seq[:, -1]

        # Run this reshaped RNN output through the decoder to get
        # output of shape (batch_size, vocab_size)
        output_distribution = self.decoder(last_output)

        # Return decoded, a distribution over the output vocabulary for
        # each sequence in the batch.
        # Shape: (batch_size, vocab_size)
        return output_distribution
