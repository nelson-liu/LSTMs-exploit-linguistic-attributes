from __future__ import unicode_literals
import os

from numpy.testing import assert_allclose
from lstms_exploit_linguistic_attributes.embedding_rnn import EmbeddingRNN
from lstms_exploit_linguistic_attributes.utils import sort_batch_by_length
import torch

from .common.test_case import ReproducibleTestCase


class TestEmbeddingRNN(ReproducibleTestCase):
    def test_embedding_forward(self):
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        # Data setup
        vocab_size = 1000
        batch_size = 5
        sequence_length = 20
        input_data = torch.LongTensor(batch_size, sequence_length).random_(
            0, vocab_size)
        input_lengths = torch.LongTensor(batch_size).random_(
            0, sequence_length)
        input_data = input_data.to(device)
        input_lengths = input_lengths.to(device)

        # Model parameters
        embedding_size = 5
        hidden_size = 5
        num_layers = 2
        dropout = 0.0
        save_path = os.path.join(self.test_dir, "test_model")

        for rnn_type in ["LSTM", "GRU", "RNN_TANH", "RNN_RELU"]:
            for tied in [True, False]:
                model = EmbeddingRNN(
                    rnn_type=rnn_type,
                    vocab_size=vocab_size,
                    embedding_size=embedding_size,
                    hidden_size=hidden_size,
                    num_layers=num_layers,
                    batch_first=True,
                    dropout=dropout,
                    tied=tied)
                model.to(device)

                output_one = model.forward(input_data)
                # Serialize model to disk
                torch.save(model, save_path)
                # Load the model.
                with open(save_path, "rb") as model_file:
                    model = torch.load(model_file)
                model.to(device)
                output_two = model.forward(input_data)
                assert_allclose(output_one.to("cpu").detach().numpy(),
                                output_two.to("cpu").detach().numpy())

                (sorted_input_data,
                 sorted_input_lengths,
                 restoration_indices, _) = sort_batch_by_length(
                     input_data, input_lengths)
                output_one = model.forward(
                    sorted_input_data, sorted_input_lengths).index_select(
                        0, restoration_indices)
                # Serialize model to disk
                torch.save(model, save_path)
                # Load the best saved model.
                with open(save_path, "rb") as model_file:
                    model = torch.load(model_file)
                model.to(device)
                output_two = model.forward(
                    sorted_input_data, sorted_input_lengths).index_select(
                        0, restoration_indices)

                assert_allclose(output_one.to("cpu").detach().numpy(),
                                output_two.to("cpu").detach().numpy())
